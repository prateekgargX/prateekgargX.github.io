---
---

@inproceedings{garg2025masked,
  title={Masked Diffusion Models are secretly Learned-Order Autoregressive Models},
  author={Garg, Prateek and Kohli, Bhavya and Sarawagi, Sunita},
  booktitle={EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)},
  url={https://openreview.net/pdf?id=zXO7Af8DZd},
  year={2025}
}

@inproceedings{garg2025from,
  abstract = {Algorithmic Recourse provides recommendations to individuals who are adversely impacted by automated model decisions, on how to alter their profiles to achieve a favorable outcome. Effective recourse methods must balance three conflicting goals: proximity to the original profile to minimize cost, plausibility for realistic recourse, and validity to ensure the desired outcome. We show that existing methods train for these objectives separately and then search for recourse through a joint optimization over the recourse goals during inference, leading to poor recourse recommendations. We introduce GenRe, a generative recourse model designed to train the three recourse objectives jointly. Training such generative models is non-trivial due to lack of direct recourse supervision. We propose efficient ways to synthesize such supervision and further show that GenRe's training leads to a consistent estimator. Unlike most prior methods, that employ non-robust gradient descent based search during inference, GenRe simply performs a forward sampling over the generative model to produce minimum cost recourse, leading to superior performance across multiple metrics. We also demonstrate GenRe provides the best trade-off between cost, plausibility and validity, compared to state-of-art baselines. We release anonymized code at: https://anonymous.4open.science/r/GenRe-BD71},
  author = {Garg, Prateek and Nagalapatti, Lokesh and Sarawagi, Sunita},
  booktitle = {The Thirteenth International Conference on Learning Representations},
  title = {From Search to Sampling: Generative Models for Robust Algorithmic Recourse},
  url = {https://openreview.net/forum?id=NtwFghsJne},
  year = {2025},
}

@article{Utilizing_Radio_Nasser_2023,
  abstract = {Graph neural networks (GNNs) present a promising alternative to CNNs and transformers in certain image processing applications due to their parameter-efficiency in modeling spatial relationships. Currently, a major area of research involves the converting non-graph input data for GNN-based models, notably in scenarios where the data originates from images. One approach involves converting images into nodes by identifying significant keypoints within them. Super-Retina, a semi-supervised technique, has been utilized for detecting keypoints in retinal images. However, its limitations lie in the dependency on a small initial set of ground truth keypoints, which is progressively expanded to detect more keypoints. Having encountered difficulties in detecting consistent initial keypoints in brain images using SIFT and LoFTR, we proposed a new approach: radiomic feature-based keypoint detection. Demonstrating the anatomical significance of the detected keypoints was achieved by showcasing their efficacy in improving registration processes guided by these keypoints. Subsequently, these keypoints were employed as the ground truth for the keypoint detection method (LK-SuperRetina). Furthermore, the study showcases the application of GNNs in image matching, highlighting their superior performance in terms of both the number of good matches and confidence scores. This research sets the stage for expanding GNN applications into various other applications, including but not limited to image classification, segmentation, and registration.},
  author = {Nasser, Sahar Almahfouz and Pathak, Shashwat and Singhal, Keshav and Meena, Mohit and Gupte, Nihar and Chinmaya, Ananya and Garg, Prateek and Sethi, Amit},
  eprint = {2311.18281v1},
  eprintclass = {eess.IV},
  eprinttype = {arxiv},
  month = {11},
  title = {Utilizing Radiomic Feature Analysis For Automated MRI Keypoint Detection: Enhancing Graph Applications},
  url = {http://arxiv.org/abs/2311.18281v1},
  year = {2023},
}

@inproceedings{anonymous2024speeding,
  abstract = {A majority of recent developments in neural architecture search (NAS) have been aimed at decreasing the computational cost of various techniques without affecting their final performance. Towards this goal, several low-fidelity and performance prediction methods have been considered, including those that train only on subsets of the training data. In this work, we present an adaptive subset selection approach to NAS and present it as complementary to state-of-the-art NAS approaches. We uncover a natural connection between one-shot NAS algorithms and adaptive subset selection and devise an algorithm that makes use of state-of-the-art techniques from both areas. We use these techniques to substantially reduce the runtime of DARTS-PT (a leading one-shot NAS algorithm), as well as BOHB and DEHB (leading multi-fidelity optimization algorithms), with minimal sacrifice to accuracy. In experiments, we find architectures on CIFAR-10 that give 5% increase in performance over DARTS-PT while reducing the time required by more than 8 times. Our results are consistent across multiple datasets, and towards full reproducibility, we release all our code at \url{https://anonymous.4open.science/r/SubsetSelection_NAS-87B3}},
  author = {C, Vishak Prasad and White, Colin and Nayak, Sibasis and Jain, Paarth and Shameem, Aziz and Garg, Prateek and Ramakrishnan, Ganesh},
  booktitle = {AutoML 2024 Methods Track},
  title = {Speeding up NAS with Adaptive Subset Selection},
  url = {https://openreview.net/forum?id=hRqiQ2i5ps},
  year = {2024},
}
